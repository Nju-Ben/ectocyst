{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Computer Vision\n",
    "## Assigment 6\n",
    "\n",
    "---\n",
    "\n",
    "This assignment contains Tensorflow programming exercises.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Install Tensorflow \n",
    "Follow the directions on https://www.tensorflow.org/install/ to install Tensorflow on your computer.\n",
    "\n",
    "Note: You will not need GPU support for this assignment so don't worry if you don't have one. Furthermore, installing with GPU support is often more difficult to configure so it is suggested that you install the CPU only version. However, if you have a GPU and would like to install GPU support feel free to do so at your own risk :)\n",
    "\n",
    "Note: On windows, Tensorflow is only supported in python3, so you will need to install python3 for this assignment.\n",
    "\n",
    "Run the following cell to verify your instalation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Hello, TensorFlow!'\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('Hello, TensorFlow!')\n",
    "sess = tf.Session()\n",
    "print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Downloading CIFAR10\n",
    "Download the CIFAR10 dataset (http://www.cs.toronto.edu/~kriz/cifar.html). You will need the python version: http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz \n",
    "\n",
    "Extract the data to ./data\n",
    "Once extracted run the following cell to view a few example images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/batches.meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2cb472ad3f16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# training and testing data that will be used in the following problems\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m# display some example images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2cb472ad3f16>\u001b[0m in \u001b[0;36mgetData\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;31m# loads all training and testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'./data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UTF-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/batches.meta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mb'label_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtrainData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-2cb472ad3f16>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/batches.meta'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# unpickles raw data files\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    import sys\n",
    "    with open(file, 'rb') as fo:\n",
    "        if sys.version_info[0] < 3:\n",
    "            dict = pickle.load(fo)\n",
    "        else:\n",
    "           dict = pickle.load(fo, encoding='bytes') \n",
    "    return dict\n",
    "\n",
    "# loads data from a single file\n",
    "def getBatch(file):\n",
    "    dict = unpickle(file)\n",
    "    data = dict[b'data'].reshape(-1,3,32,32).transpose(0,2,3,1)\n",
    "    labels = np.asarray(dict[b'labels'], dtype=np.int64)\n",
    "    return data,labels\n",
    "\n",
    "# loads all training and testing data\n",
    "def getData(path='./data'):\n",
    "    classes = [s.decode('UTF-8') for s in unpickle(path+'/batches.meta')[b'label_names']]\n",
    "    \n",
    "    trainData, trainLabels = [], []\n",
    "    for i in range(5):\n",
    "        data, labels = getBatch(path+'/data_batch_%d'%(i+1))\n",
    "        trainData.append(data)\n",
    "        trainLabels.append(labels)\n",
    "    trainData = np.concatenate(trainData)\n",
    "    trainLabels = np.concatenate(trainLabels)\n",
    "    \n",
    "    testData, testLabels = getBatch(path+'/test_batch')\n",
    "    return classes, trainData, trainLabels, testData, testLabels\n",
    "\n",
    "# training and testing data that will be used in the following problems\n",
    "classes, trainData, trainLabels, testData, testLabels = getData()\n",
    "\n",
    "# display some example images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "for i in range(14):\n",
    "    plt.subplot(2,7,i+1)\n",
    "    plt.imshow(trainData[i])\n",
    "    plt.title(classes[trainLabels[i]])\n",
    "plt.show()\n",
    "\n",
    "print ('train shape: ' + str(trainData.shape) + ', ' + str(trainLabels.shape))\n",
    "print ('test shape : ' + str(testData.shape) + ', ' + str(testLabels.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some helper functions that will be used in the following problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a generator for batches of data\n",
    "# yields data (batchsize, 3, 32, 32) and labels (batchsize)\n",
    "# if shuffle, it will load batches in a random order\n",
    "def DataBatch(data, label, batchsize, shuffle=True):\n",
    "    n = data.shape[0]\n",
    "    if shuffle:\n",
    "        index = np.random.permutation(n)\n",
    "    else:\n",
    "        index = np.arange(n)\n",
    "    for i in range(int(np.ceil(n/batchsize))):\n",
    "        inds = index[i*batchsize : min(n,(i+1)*batchsize)]\n",
    "        yield data[inds], label[inds]\n",
    "\n",
    "# tests the accuracy of a classifier\n",
    "def test(testData, testLabels, classifier):\n",
    "    batchsize=50\n",
    "    correct=0.\n",
    "    for data,label in DataBatch(testData,testLabels,batchsize):\n",
    "        prediction = classifier(data)\n",
    "        #print (prediction)\n",
    "        correct += np.sum(prediction==label)\n",
    "    return correct/testData.shape[0]*100\n",
    "\n",
    "# a sample classifier\n",
    "# given an input it outputs a random class\n",
    "class RandomClassifier():\n",
    "    def __init__(self, classes=10):\n",
    "        self.classes=classes\n",
    "    def __call__(self, x):\n",
    "        return np.random.randint(self.classes, size=x.shape[0])\n",
    "\n",
    "randomClassifier = RandomClassifier()\n",
    "print ('Random classifier accuracy: %f'%test(testData, testLabels, randomClassifier))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Confusion Matirx\n",
    "Here you will implement a test script that computes the confussion matrix for a classifier.\n",
    "The matrix should be nxn where n is the number of classes.\n",
    "Entry M[i,j] should contain the number of times an image of class i was classified as class j.\n",
    "M should be normalized such that each row sums to 1.\n",
    "\n",
    "Hint: see the function test() above for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion(testData, testLabels, classifier):\n",
    "    \"\"\"your code here\"\"\"\n",
    "    return M\n",
    "\n",
    "\n",
    "def VisualizeConfussion(M):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    plt.imshow(M)#, vmin=0, vmax=1)\n",
    "    plt.xticks(np.arange(len(classes)), classes, rotation='vertical')\n",
    "    plt.yticks(np.arange(len(classes)), classes)\n",
    "    plt.show()\n",
    "\n",
    "M = confusion(testData, testLabels, randomClassifier)\n",
    "VisualizeConfussion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: K-Nearest Neighbors (KNN)\n",
    "Here you will implemnet a simple knn classifer. The distance metric is euclidian in pixel space. k refers to the number of neighbors involved in voting on the class.\n",
    "\n",
    "Hint: you may want to use: sklearn.neighbors.KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "class KNNClassifer():\n",
    "    def __init__(self, k=3):\n",
    "        # k is the number of neighbors involved in voting\n",
    "        \"\"\"your code here\"\"\"\n",
    "        \n",
    "    def train(self, trainData, trainLabels):\n",
    "        \"\"\"your code here\"\"\"\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # this method should take a batch of images (batchsize, 32, 32, 3) and return a batch of prediction (batchsize)\n",
    "        # predictions should be int64 values in the range [0,9] corrisponding to the class that the image belongs to\n",
    "        \"\"\"your code here\"\"\"\n",
    "\n",
    "\n",
    "# test your classifier with only the first 100 training examples (use this while debugging)\n",
    "# note you should get around 10-20% accuracy\n",
    "knnClassiferX = KNNClassifer()\n",
    "knnClassiferX.train(trainData[:100], trainLabels[:100])\n",
    "print ('KNN classifier accuracy: %f'%test(testData, testLabels, knnClassiferX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test your classifier with all the training examples (This may take a while)\n",
    "# note you should get around 30% accuracy\n",
    "knnClassifer = KNNClassifer()\n",
    "knnClassifer.train(trainData, trainLabels)\n",
    "print ('KNN classifier accuracy: %f'%test(testData, testLabels, knnClassifer))\n",
    "\n",
    "# display confusion matrix for your KNN classifier with all the training examples\n",
    "M = confusion(testData, testLabels, knnClassifer)\n",
    "VisualizeConfussion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5: Principal Component Analysis (PCA) K-Nearest Neighbors (KNN)\n",
    "Here you will implemnet a simple knn classifer in PCA space.\n",
    "You should implement PCA yourself using svd (you may not use sklearn.decomposition.PCA\n",
    "or any other package that directly implements PCA transofrmations\n",
    "\n",
    "Hint: Don't forget to apply the same normalization at test time.\n",
    "\n",
    "Note: you should get similar accuracy to above, but it should run faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "class PCAKNNClassifer():\n",
    "    def __init__(self, components=25, k=3):\n",
    "        \"\"\"your code here\"\"\"\n",
    "\n",
    "        \n",
    "    def train(self, trainData, trainLabels):\n",
    "        \"\"\"your code here\"\"\"\n",
    "\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        \"\"\"your code here\"\"\"\n",
    "\n",
    "        \n",
    "    \n",
    "# test your classifier with only the first 100 training examples (use this while debugging)\n",
    "pcaknnClassiferX = PCAKNNClassifer()\n",
    "pcaknnClassiferX.train(trainData[:100], trainLabels[:100])\n",
    "print ('PCA-KNN classifier accuracy: %f'%test(testData, testLabels, pcaknnClassiferX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# test your classifier with all the training examples (This may take a few minutes)\n",
    "pcaknnClassifer = PCAKNNClassifer()\n",
    "pcaknnClassifer.train(trainData, trainLabels)\n",
    "print ('KNN classifier accuracy: %f'%test(testData, testLabels, pcaknnClassifer))\n",
    "\n",
    "# display the confusion matrix\n",
    "M = confusion(testData, testLabels, pcaknnClassifer)\n",
    "VisualizeConfussion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep learning\n",
    "Below is some helper code to train your deep networks\n",
    "\n",
    "Hint: see https://www.tensorflow.org/get_started/mnist/pros or https://www.tensorflow.org/get_started/mnist/beginners for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# base class for your Tensorflow networks. It implements the training loop (train) and prediction(__call__)  for you.\n",
    "# You will need to implement the __init__ function to define the networks structures in the following problems\n",
    "class TFClassifier():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, trainData, trainLabels, epochs=1, batchsize=50):\n",
    "        self.prediction = tf.argmax(self.y,1)\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y_, logits=self.y))\n",
    "        self.train_step = tf.train.AdamOptimizer(1e-4).minimize(self.cross_entropy)\n",
    "        self.correct_prediction = tf.equal(self.prediction, self.y_)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for i, (data,label) in enumerate(DataBatch(trainData, trainLabels, batchsize, shuffle=True)):\n",
    "                _, acc = self.sess.run([self.train_step, self.accuracy], feed_dict={self.x: data, self.y_: label})\n",
    "                #if i%100==99:\n",
    "                #    print ('%d/%d %d %f'%(epoch, epochs, i, acc))\n",
    "                    \n",
    "            print ('testing epoch:%d accuracy: %f'%(epoch+1, test(testData, testLabels, self)))\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return self.sess.run(self.prediction, feed_dict={self.x: x})\n",
    "\n",
    "# helper function to get weight variable\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# helper function to get bias variable\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# example linear classifier\n",
    "class LinearClassifer(TFClassifier):\n",
    "    def __init__(self, classes=10):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None,32,32,3]) # input batch of images\n",
    "        self.y_ = tf.placeholder(tf.int64, shape=[None]) # input labels\n",
    "\n",
    "        # model variables\n",
    "        self.W = weight_variable([32*32*3,classes])\n",
    "        self.b = bias_variable([classes])\n",
    "\n",
    "        # linear operation\n",
    "        self.y = tf.matmul(tf.reshape(self.x,(-1,32*32*3)),self.W) + self.b\n",
    "        \n",
    "# test the example linear classifier (note you should get around 20-30% accuracy)\n",
    "linearClassifer = LinearClassifer()\n",
    "linearClassifer.train(trainData, trainLabels, epochs=20)\n",
    "\n",
    "# display confusion matrix\n",
    "M = confusion(testData, testLabels, linearClassifer)\n",
    "VisualizeConfussion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6: Multi Layer Perceptron (MLP)\n",
    "Here you will implement an MLP. The MLP shoud consist of 3 linear layers (matrix multiplcation and bias offset) that map to the following feature dimensions:\n",
    "\n",
    "32x32x3 -> hidden\n",
    "\n",
    "hidden -> hidden\n",
    "\n",
    "hidden -> classes\n",
    "\n",
    "The first two linear layers should be followed with a ReLU nonlinearity. The final layer should not have a nonlinearity applied as we desire the raw logits output (see: the documentation for tf.nn.sparse_softmax_cross_entropy_with_logits used in the training)\n",
    "\n",
    "The final output of the computation graph should be stored in self.y as that will be used in the training.\n",
    "\n",
    "Hint: see the example linear classifier\n",
    "\n",
    "Note: you should get around 50% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MLPClassifer(TFClassifier):\n",
    "    def __init__(self, classes=10, hidden=100):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None,32,32,3]) # input batch of images\n",
    "        self.y_ = tf.placeholder(tf.int64, shape=[None]) # input labels\n",
    "\n",
    "        \"\"\"your code here\"\"\"\n",
    "\n",
    "        \n",
    "\n",
    "# test your MLP classifier (note you should get around 50% accuracy)\n",
    "mlpClassifer = MLPClassifer()\n",
    "mlpClassifer.train(trainData, trainLabels, epochs=20)\n",
    "\n",
    "# display confusion matrix\n",
    "M = confusion(testData, testLabels, mlpClassifer)\n",
    "VisualizeConfussion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Convolutional Neural Netork (CNN)\n",
    "Here you will implement a CNN with the following architecture:\n",
    "\n",
    "ReLU( Conv(kernel_size=4x4 stride=2, output_features=n) )\n",
    "\n",
    "ReLU( Conv(kernel_size=4x4 stride=2, output_features=n*2) )\n",
    "\n",
    "ReLU( Conv(kernel_size=4x4 stride=2, output_features=n*4) )\n",
    "\n",
    "Linear(output_features=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, W, stride=2):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, stride, stride, 1], padding='SAME')\n",
    "\n",
    "class CNNClassifer(TFClassifier):\n",
    "    def __init__(self, classes=10, n=16):\n",
    "        self.sess = tf.Session()\n",
    "\n",
    "        self.x = tf.placeholder(tf.float32, shape=[None,32,32,3]) # input batch of images\n",
    "        self.y_ = tf.placeholder(tf.int64, shape=[None]) # input labels\n",
    "\n",
    "        \"\"\"your code here\"\"\"\n",
    "\n",
    "\n",
    "# test your CNN classifier (note you should get around 65% accuracy)\n",
    "cnnClassifer = CNNClassifer()\n",
    "cnnClassifer.train(trainData, trainLabels, epochs=20)\n",
    "\n",
    "# display confusion matrix\n",
    "M = confusion(testData, testLabels, cnnClassifer)\n",
    "VisualizeConfussion(M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Have you accomplished all parts of your assignment? What concepts did you used or learned in this assignment? What difficulties have you encountered? Explain your result for each section. Please wirte one or two short paragraph in the below Markdown window (double click to edit)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**** Your Conclusion: ****\n",
    "\n",
    "--\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further reference\n",
    "To see how state of the art deep networks do on this dataset see: https://github.com/tensorflow/models/tree/master/research/resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
